<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Categorical Perception of ASL - Methods</title>
        <link rel="icon" href="./favicon.ico" type="image/x-icon">
        <link rel="stylesheet" href="./css/style.css">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Lexend:wght@100;200;300;400;500;600;700;800;900&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/vue/2.7.14/vue.js"></script>
        <script src="./script/script.js" defer></script>
    </head>

    <body>
        <div id="site-area">
            <header id="top">        
                
                <div id="menu-box">
                    <input type="checkbox" id="hamburger-input" class="burger-shower">
                    <label id="hamburger-icon" for="hamburger-input">
                        <i class="fa-solid fa-bars" id="open-menu"></i>
                        <i class="fa-solid fa-x" id="close-menu"></i> 
                    </label>   

                    <nav id="dropdown-menu"> 
                        <ul>
                            <li><a href="index.html">Home</a></li>
                            <li><a href="about.html">About</a></li>
                            <li><a href="introduction.html">Introduction</a></li>
                            <li><a href="methods.html">Methods</a></li>
                            <li><a href="results.html">Results</a></li>
                            <li><a href="discussion.html">Discussion</a></li>
                            <li><a href="references.html">References &amp; Appendices</a></li>
                            <li><a href="https://github.com/SRDMoss/cp-asl" target="_blank">Github</a></li>
                        </ul>
                    </nav> 
                </div>
                
                <section id="full-title">
                    <h1 id="page-title">Categorical Perception of Palm Orientation in American Sign Language</h1>   
                    <span id="byline">by Stephen Richard DeVilbiss Moss</span>              
                </section>                            
            </header>
            
            <main>
                <h2>Methods</h2>

                <article>
                    <h3>Participants</h3>
                    <p>Sixty-eight individuals participated in the study. All participants were adults. Three participants were excluded for failure to follow directions, five were excluded due 
                        to disqualifying language experience. Of the remaining sixty participants, 21 individuals reported full hearing loss, 4 individuals reported partial hearing loss, and 35 
                        reported no hearing loss. Participants were recruited in two experience groups: native signers and sign naïve individuals. Within each group, participants were randomly 
                        assigned to one of two primed conditions.</p>
                    <p>All participants were informed of the nature of the experiment in their preferred language (ASL or English), and individuals consented to participate. Each participant was 
                        compensated either &#36;10 cash or with course credit. Participants were recruited at both University of Maryland, College Park and Gallaudet University. </p>
                </article>

                <article>
                    <figure>
                        <img class="media" src="assets/img/Hand-009.jpg" alt="A wooden model hand in front of a neutral background.">
                        <figcaption>One example of preliminary tokens used during the experimental development stages.</figcaption>
                    </figure> 

                    <figure>
                        <video class="media" src="assets/video/050.mp4" preload autoplay loop>This video won't play. Shucks.</video>
                        <figcaption>Movement Epenthesis: Without the movement in the beginning, the video would seem un-natural, as if the begining were cut off.</figcaption>
                    </figure>
                </article>

                <h2>Linguistic Priming Videos</h2>
                
                <figure>
                    <video class="media" src="assets/video/DepictingShort.mp4" preload controls>This video doesn't work in your browser.</video>
                    <figcaption>Depicting Prime: This video was played to prime participants of the <q>Depicting</q> language context.</figcaption>
                </figure>

                <figure>
                    <video class="media" src="assets/video/Lexical.mp4" preload controls>This video doesn't work in your browser.</video>
                    <figcaption>Depicting Prime: This video was played to prime participants of the <q>Lexical</q> language context.</figcaption>
                </figure>

                <article>                
                    <h3>Materials</h3>
                    <p>All participants used the Dell XPS 18 Portable All-In-One Computer to complete the experiment. Participants used the touch-screen feature to respond to test trials and 
                        keyboard to enter appropriate information in the demographic questionnaire. All stimuli were modeled by pre-lingually deaf native ASL signers. Token stimuli were filmed 
                        using two Logitech HD Webcam C920 cameras. The first camera—located in front of the signer—filmed the token used in the experiment. The continuum consisted of eleven signs 
                        along a 100 degree range at 10 degree intervals. (See Appendix B) From the beginning of its study, spoken language CP studies have attempted to make tokens more language-like 
                        by presenting the phones in question with another neutral phone as an anchor (Liberman et. al., 1967). In order to simulate this method of triggering language-like perception, 
                        a movement-epenthesis was filmed as part of the sign. The model began with his right hand resting at his side and then moved into the signing position for each token. A black 
                        post was placed in front of the signer to ensure the location of the sign would be the same for each token.</p>
                    <p>In order to improve accuracy of intervals, a second camera was oriented above the signer and a birds-eye photo of each token was associated with its video component. Each 
                        photo angle was then measured in relation to the base of the post (See Appendix C). The model was filmed signing each token 12 times and the most accurate tokens were 
                        selected for use in the experiment. </p>
                    <p>The model for the primes was a different pre-lingually deaf, native signer. Primes were designed to use this same continuum and trigger different numbers of categories 
                        within. The <q>Street</q>  prime was meant to trigger a schema of the continuum that indicated two categories: <q>To get to the store, drive forward [Cat 1] to the intersection, 
                        then go left [Cat 2]. The store will be on your right.</q> The <q>Clock</q> prime was meant to trigger a schema of the continuum that included four categories: <q>A teacher is giving 
                        a lesson to children by having them gather around a large model clock in front of her. The teacher then explains that when the hand is positioned in this position[Cat 1], 
                        the time is twelve o&#39;clock; when the hand is positioned in this position[Cat 2], the time is nine o&#39;clock; when the hand is positioned in this position [Cat 3] the time is 
                        eleven o&#39;clock; and when the hand is positioned in this position [Cat 4] the time is ten o&#39;clock</q> While the ASL representation of the clock prime is significantly more 
                        succinct than the English shown here, the full prime was only shown at the start of each task. Throughout the tasks, only the second sentence was shown reduce the time 
                        between trials. </p>
                </article>

                <article>
                    <h3>Procedure</h3>
                    <p>Two tasks were used to assess the category locations and discriminability along the continuum. Instructions for both tasks were included within the application in English as 
                        well as ASL. Participants were given opportunities to ask questions throughout. </p>
                    <p>In the identification task, participants were shown a prime, either <q>Street</q> or <q>Clock,</q> which was labeled as <q>instructor.</q> At each trial, participants were shown one video 
                        token from the continuum labeled <q>student</q> at a random location on the screen, followed by the array of photo tokens (See Appendix B). Participants were then to select the 
                        photo which best matches the sign in the instructor&#39;s story that the student was attempting. The primes were shown twelve times each in random order. The primes were 
                        displayed after every twelve trials.</p>
                    <p>The discrimination task followed ABX trial format. Three black squares were displayed on the screen. In each box a token was displayed in the left then right then center 
                        separated by .25 second interstimulus interval. The first two tokens shown were separated one step on the continuum and the third token was the same as either the first 
                        or second token displayed. Each interval was presented twelve times, in three randomized batches. In each batch, each interval was generated in four patterns: ABA, ABB, 
                        BAA, BAB and the full batch was randomized within. The participants then filled out a demographic questionnaire within the computer application. </p>
                </article>

            </main>
            <footer id="bottom">
                <hr>
                    &copy; 2014, 2015, 2023, Stephen Moss.
                <hr>
            </footer>

            <!-- Modal -->
            <div id="mediaModal" class="modal">                
                <div class="modal-content" id="modalContent">
                    <!-- Dynamic content here -->
                </div>
                <span class="close-btn" id="closeModal">&times;</span>
            </div>
        </div>
    </body>
</html>